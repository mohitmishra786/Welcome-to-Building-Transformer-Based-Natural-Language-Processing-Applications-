{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Transformer Decoder\n",
    "\n",
    "In this notebook, you'll complete your examination of the architecture with a look at the decoder and masked multi-headed attention.  Finally, you'll run inference for the NMT English-German task.\n",
    "\n",
    "**[3.1 Overview](#3.1-Overview)<br>**\n",
    "**[3.2 Masked Multi-Head Attention](#3.2-Masked-Multi-Head-Attention)<br>**\n",
    "**[3.3 References](#3.3-References)**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder operates in a similar way to the encoder, but generates one word at a time, from left to right. It attends not only to the other previously generated words, but also to the final representations generated by the encoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/enc_dec_2.png\" width=\"800\"></center>\n",
    "<center> Figure 6. Decoder block illustration. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the encoding phase, the decoding phase starts. The decoder has a very similar structure to the encoder. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, there are residual connections around each of the sub-layers, followed by layer normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TransformerDecoder` class is shown below. For simplicity, lines related to values read from `args` have been removed, as well as some normalization and data transpositions. Please refer to the [original implementation](https://github.com/NVIDIA/DeepLearningExamples/blob/8c3514071275b2805b29372f6dabe515d431416f/PyTorch/Translation/Transformer/fairseq/models/transformer.py#L298) to see these lines.\n",
    "\n",
    "Note that `self.layers` is a `nn.ModuleList`, comprised of `args.decoder_layers` (default 6) copies of `TransformerDecorderLayer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import PIL\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Optional, Dict\n",
    "import math, copy, time\n",
    "\n",
    "from encoder_demos.demo_fairseq.models.fairseq_model import BaseFairseqModel, FairseqDecoder, FairseqEncoder\n",
    "from encoder_demos.demo_fairseq.models.fairseq_incremental_decoder import FairseqIncrementalDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Optional, Dict\n",
    "class TransformerDecoder(FairseqIncrementalDecoder):\n",
    "    \"\"\"Transformer decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, args, embed_tokens, no_encoder_attn=False, left_pad=False):\n",
    "        super().__init__()\n",
    "        self.dropout = args.dropout\n",
    "        self.share_input_output_embed = args.share_decoder_input_output_embed\n",
    "        self.fuse_dropout_add = args.fuse_dropout_add\n",
    "        self.fuse_relu_dropout = args.fuse_relu_dropout\n",
    "\n",
    "        embed_dim = embed_tokens.embedding_dim\n",
    "        padding_idx = embed_tokens.padding_idx\n",
    "        self.max_target_positions = args.max_target_positions\n",
    "\n",
    "        self.embed_tokens = embed_tokens\n",
    "        self.embed_scale = math.sqrt(embed_dim)\n",
    "        self.embed_positions = PositionalEmbedding(\n",
    "            args.max_target_positions, embed_dim, padding_idx,\n",
    "            left_pad=left_pad,\n",
    "            learned=args.decoder_learned_pos,\n",
    "        ) if not args.no_token_positional_embeddings else None\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layers.extend([\n",
    "            TransformerDecoderLayer(args, no_encoder_attn)\n",
    "            for _ in range(args.decoder_layers)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                prev_output_tokens: Tensor,\n",
    "                encoder_out: Tensor,\n",
    "                encoder_padding_mask: Tensor,\n",
    "                incremental_state: Optional[Dict[str, Dict[str, Tensor]]]=None):\n",
    "        # embed positions\n",
    "        positions = self.embed_positions(\n",
    "            prev_output_tokens,\n",
    "            incremental_state=incremental_state,\n",
    "        ) if self.embed_positions is not None else None\n",
    "\n",
    "        if incremental_state is not None:\n",
    "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
    "            if positions is not None:\n",
    "                positions = positions[:, -1:]\n",
    "\n",
    "        # embed tokens and positions\n",
    "        x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n",
    "        if positions is not None:\n",
    "            x += positions\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        # The tensor needs to copy transposed because\n",
    "        # fused dropout is not capable of handing strided data\n",
    "        if self.fuse_dropout_add :\n",
    "            x = x.transpose(0, 1).contiguous()\n",
    "        else :\n",
    "            x = x.transpose(0, 1)\n",
    "        attn = None\n",
    "\n",
    "        # decoder layers\n",
    "        for layer in self.layers:\n",
    "            x, attn = layer(\n",
    "                x,\n",
    "                encoder_out,\n",
    "                encoder_padding_mask if encoder_padding_mask.any() else None,\n",
    "                incremental_state,\n",
    "            )\n",
    "\n",
    "\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `TransformerDecoderLayer` is a copy of the TransformerDecoderLayer class defined [here](https://github.com/NVIDIA/DeepLearningExamples/blob/8c3514071275b2805b29372f6dabe515d431416f/PyTorch/Translation/Transformer/fairseq/models/transformer.py#L487). The full implementation includes optional layer normalization, but we have removed this for the sake of simplicity.\n",
    "\n",
    "Looking at the `forward` method, the embedded tokens `x` are passed through the self-attention mechanism and then later `fc1` and `fc2`, in a very similar way to the `TransformerEncoderLayer`. The difference in the decoder is that there is an attention process between the self-attention and the fully connected layers. This introduces the multi-head attention which decodes `x`, eventually producing intelligible output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each step in the decoding phase outputs an element from the output sequence, which is the English translation sentence in our case. The encoder starts by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder side of the Figure 1 would perform the following operations successively: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1_out = OutputEmbedding512 + PositionEncoding512\n",
    "\n",
    "Step2_Mask = masked_multihead_attention(Step1_out)\n",
    "\n",
    "Step2_Norm1 = layer_normalization(Step2_Mask) + Step1_out\n",
    "\n",
    "Step2_Multi = multihead_attention(Step2_Norm1 + out_enc) +  Step2_Norm1\n",
    "\n",
    "Step2_Norm2 = layer_normalization(Step2_Multi) + Step2_Multi\n",
    "\n",
    "Step3_FNN = FNN(Step2_Norm2)\n",
    "\n",
    "Step3_Norm = layer_normalization(Step3_FNN) + Step2_Norm2\n",
    "\n",
    "out_dec = Step3_Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Masked Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed the \"masked multi-head attention\" layer in the decoder. Self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position <i>i</i> can depend only on the known outputs at positions less than <i>i</i>. In other words, masked multi-head attention is applied to prevent future words to be part of the attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.tril(np.ones((1,10,10))).astype('uint8')    \n",
    "    return torch.as_tensor(subsequent_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsequent_mask(10)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What masking does is to zero-out the similarities between words and the words that appear after the source words (\"in the future\"). It simply removes such information, so it cannot be used by the model, only similarity to the preceding words is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "cmap = plt.cm.GnBu_r\n",
    "plt.imshow(subsequent_mask(10)[0], cmap=cmap)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The animation below illustrates how the Transformer is applied to machine translation. The Transformer starts by generating initial representations, or embeddings, for each word. These are represented by the unfilled circles. Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoderflow](https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s1600/transform20fps.gif)\n",
    "<center> Figure 7. Transformer step-by-step sequence English-to-French translation. </center>\n",
    "\n",
    " Credit: [Google Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "2. [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "3. [Word2Vec](https://arxiv.org/pdf/1310.4546.pdf)\n",
    "4. [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "5. [fastText](https://fasttext.cc/)\n",
    "6. [Neural Machine Translation of Rare Words with Subword Units](https://www.aclweb.org/anthology/P16-1162)\n",
    "7. [The Annotated Transformer - Harvard NLP](https://nlp.seas.harvard.edu/2018/04/03/attention.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've completed the final Tranformer notebook!  \n",
    "You've learned \n",
    "* Decoders have three sublayers\n",
    "* Decoders attend previously generated words and the final representations generated by the encoder\n",
    "* Decoders use masked multi-head attention to prevent future words from being included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "186px",
    "left": "619px",
    "top": "238px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
