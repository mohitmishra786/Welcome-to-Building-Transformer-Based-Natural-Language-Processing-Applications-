# Welcome-to-Building-Transformer-Based-Natural-Language-Processing-Applications-


**Update! Course has been updated to NVIDIA NeMo v1.0!**

## Learning Objectives ##

Learn how to use natural language processing (NLP) Transformer-based models for "text classification" tasks, such as identifying specific types of articles from within a large library of articles or abstracts. You'll also learn how to leverage Transformer-based models for "named entity recognition (NER)" tasks, and learn how to analyze various model features, constraints, and characteristics to determine which are best suited for a particular use case based on metrics, domain specificity, and available resources

***You'll learn how to***:

- Construct a Transformer neural network in PyTorch for language translation
- Build a text classification project using pre-trained BERT-variant models to classify abstracts
- Build a named-entity recognition (NER) project using pre-trained domain-specific models to identify disease names in text
- Deploy an NLP inference project to NVIDIA Triton

***Prerequisites***

* In order to be successful in this course, you will need the following:
* Python programming experience
* Basic understanding of neural networks
* Fundamental understanding of a deep learning framework such as TensorFlow or PyTorch
